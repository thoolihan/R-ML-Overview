Machine Learning in R
========================================================
author: Tim Hoolihan (@thoolihan)
date: 2018.05.04
autosize: true
```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
```

Tim Hoolihan
========================================================
- Organizer of the Cleveland R User Group
- Sr. Director of Data Science at DialogTech
- Advisory Board Member at DriveIT in Akron
- Author of a Packt Pub Video Series on Machine Learning in R
- @thoolihan on github & twitter

========================================================
![Questions? Feedback?](images/StirTrek2018-SessionSlack.png)

## We are in \#2018--purple

About the Talk
========================================================

1. Super-brief crash course in R
2. What is machine learning
3. Regression - Predicting Continuous Values
4. Classification - Predicting Discrete(Categorical) Values
5. Briefly touch on Deep Learning
6. Your Next Steps

R
========================================================
- An implementation of the S language, that has evolved on it's own
- Built-in support for Statistics
- Lots of packages for a variety of STEM domains
  - Considered a large reason for the growth of the R Community
- Integrations with a lot of systems (Most RDBMSs, Hadoop, Spark, ElasticSearch, etc)
- [r-project.org](r-project.org)

Unique Things About R Syntax
========================================================
Assignment supports `=`, but `<-` is dominant in practice

```{r}
x = 5

# above is the same as below

x <- 5
```

Unique Things About R Syntax
========================================================
Functions declared through assignment, implied return
```{r}
foo <- function(x, y) {
  return(x + y)
}

# above is the same as below

foo <- function(x, y) {
  x + y
}
```

Vectors and Vector Operations
=======================================================
```{r}
heights <- c(1.7, 1.72, 1.68, 1.6, 1.8)
length(heights)
mean(heights)
sd(heights)

```

Vectors and Vector Operations
=======================================================
```{r}
heights <- c(1.7, 1.72, 1.68, 1.6, 1.8)
feet_per_meter <- 3.28

tmp <- heights * feet_per_meter

# above is preferred and more performant than

tmp <- c()
for(height in heights){
  tmp <- c(tmp, height * feet_per_meter)
}
```

Distributions
=======================================================
```{r}
# normal distribution
heights = rnorm(500, mean = 66, sd = 3.5)
heights[1:5]
```

Histogram
=======================================================
```{r}
hist(heights, breaks = 12)
```

Let's make a related variable with some noise
=======================================================
```{r}
# uniform distrubution added to heights
armspans <- heights + runif(500, min = -3, max = 3)
armspans[1:5]
```

Data Frames
=======================================================
```{r}
people <- data.frame(height = heights, armspan = armspans)
head(people)
```

Data Frames
=======================================================
The dim function (dimensions) shows `r dim(people)[1]` observations of `r dim(people)[2]` variables
```{r}
dim(people)
```

Libraries
=======================================================
```{r}
library(ggplot2)
```

Plot Data
=======================================================
```{r}
qplot(x = height, y = armspan, data = people)
```

Formula
=======================================================
- `armspan ~ height` is a formula, can be read as "armspan by height"
```{r}
reg_form <- lm(armspan ~ height, data = people)
```

Summary of our Regression Line
=======================================================
```{r}
summary(reg_form)
```

Regression
=======================================================
- lm is a function to fit a linear model
```{r}
reg_form$coefficients
```
- Remember y = mx + b
  - armspan = `r reg_form$coefficients[['height']]` * height + `r reg_form$coefficients[['(Intercept)']]`

Apply Formula
=======================================================
```{r}
people$rline <- predict(reg_form, newdata = people)
people$rline
```

Plot Regression
=======================================================
```{r}
ggplot(data = people, aes(x = height)) +
  geom_point(aes(y = armspan)) +
  geom_line(aes(y = rline, color = 'regression line'))
```


What is Our Regression Formula/Line?
=======================================================
It describes the relationship between the response variable (armspan) and the dependent variable (height). In other words, it describes how armspan is expected to change when height changes.

How It Was Created
=======================================================
We did not explicitly program the relationship with rules, we used an alogorithm that could infer the relationship based on example data.

Does The Relationship Hold?
=======================================================
Let's assume that was real data from American adults (18-50) measured in the last 10 years...
- Is the relationship valid for modern people in France?
- Is the relationship valid for children?
- Is the relationship valid for senior citizens?
- Is the relationship valid for ancient Neanderthal men?

This Is Machine Learning
=======================================================
The model to describe the relationship can be used to to predict the response variable, based on the input variable(s). It's not a perfect predictor, but can be a useful prediction. Among other considerations, it's best that you're trying to predict on a subject that is of the same type as the data used to train the model.

As an example, our model for American adults should not be applied to children, as they were not represented in the set and have meaningful physiological differences.


So Let's Make a Prediction
=======================================================
We meet someone new with a height of 70 inches. What does our model predict their armspan is?
```{r}
new_person = list(height = 70)
predict(reg_form, newdata = new_person)
```

A Word on Data Sets
=======================================================
# When Supervised Learning...
## Train
Data used to train and tune the algorithm

## Validation
Data used to validate the model being built

## Test
Data used with final model to estimate model fit

Caret Package
=======================================================
* Name is short for **C**lassification **A**nd **RE**gression **T**raining

Contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

Regression: Predicting HP
=======================================================
```{r}
head(mtcars)
```

Select Continuous Columns
=======================================================
```{r}
x <- mtcars[, c('hp', 'mpg', 'disp', 'wt', 'qsec')]
head(x)
```

Let's Look At Correlation
=======================================================
```{r}
pairs(x)
```

Split Our Data
=======================================================
```{r}
library(caret)

training_index <- createDataPartition(x$hp, p = 0.7, list = FALSE)
train_data <- x[training_index, ]
test_data <- x[-training_index, ]  # note the '-' in front of the index

dim(train_data)
```

Create Our Model
=======================================================
```{r}
model <- train(hp ~ .,
               data = train_data,
               method = "lm")

summary(model)
```

Predict On Test Data
=======================================================
```{r}
test_data$predicted <- predict(model, test_data)

test_data$miss <- test_data$predicted - test_data$hp

test_data[, c('hp', 'predicted', 'miss')]
```

Measuring Error
=======================================================
```{r}
library(ModelMetrics)
mse(test_data$hp, test_data$predicted)
```

```{r}
mae(test_data$hp, test_data$predicted)
```

Housing Prices
=======================================================
Let's look at a larger size data set:
```{r}
houses_raw <- read.csv("data/housing/train.csv")

houses <- houses_raw[, c('LotArea', 'Neighborhood',
                         'YearBuilt', 'TotRmsAbvGrd',
                         'FullBath', 'HalfBath',
                         'GrLivArea', 'TotalBsmtSF',
                         'SalePrice')]
dim(houses)
```

Categorical Features
=======================================================
```{r}
class(houses$Neighborhood)
```
```{r}
summary(houses$Neighborhood)
```

Cross Validation
=======================================================
![kfold cross val](images/kfold.jpg)

[creative commons - wikimedia]

Cross Validation
=======================================================
```{r}
folds <- 10
trctl <- trainControl(method = "cv", number = folds, savePredictions = TRUE)

model <- train(SalePrice ~ .,
               data = houses,
               method = "lasso",  # another regression model
               trControl = trctl,
               preProcess = c('center', 'scale'))
```

Examine Housing Results
=======================================================
```{r}
mae(houses$SalePrice, model$pred$pred)
```

A Word On Centering
=======================================================
```{r}
center <- function(data) {
  return(data - mean(data))
}

center(0:10)
```

A Word On Scaling
=======================================================
```{r}
scale <- function(data) {
  data <- data - min(data)
  return(data / max(data))
}

scale(0:10)
```

Combined
=======================================================
```{r}
data <- runif(10, min = 1e4, max = 5e6)
data
```

```{r}
center(scale(data))
```

Preprocessing
=======================================================
Caret just handled that with:
```{r, eval = FALSE}
preProcess = c('center', 'scale')
```

Classification
=======================================================
iris data set
```{r}
head(iris)
```

SVM for Classification
=======================================================
Support Vector Machine
```{r}
trctl <- trainControl(method="cv", repeats=3,
                      savePredictions = TRUE)

model <- train(Species ~ .,
               data = iris,
               method = "svmRadial",
               trControl = trctl,
               preprocess = c('scale'))
```

SVM Results
=======================================================
Confusion Matrix
```{r}
#prefix with library, but ModelMetrics also has a confusion matrix function
caret::confusionMatrix(model$pred$obs, model$pred$pred)$table
```

Unsupervised Learning
=======================================================
The algorithm looks for patterns without answers. This is useful for exploring your data.

Clustering with labelled classification data before doing supervised learning, can be a great way to decide if the categories you selected may have overlap, or other issues.

Unsupervised Learning
=======================================================
Let's load up Satellite image data that could be used in a classification problem

```{r}
library(mlbench)
library(tsne)

data("Satellite")
dim(Satellite)
```

Clustering Satellite Data
=======================================================
What fields to do we have?

```{r}
names(Satellite)
```

Clustering Satellite Data
=======================================================
What does our input data look like?

```{r}
class(Satellite$x.10)
```


```{r}
summary(Satellite$x.10)
```

Clustering Satellite Data
=======================================================
What does our label data look like?

```{r}
class(Satellite$classes)
```


```{r}
summary(Satellite$classes)
```

Split into X, y
=======================================================
In preparation for projecting...

We're going to use principle components and tsne to create a 2d projection of 36 dimensional data.

```{r}
X <- Satellite[, -37]
y <- Satellite$classes
```

```{r, echo = FALSE}
X <- X[1:2000, 1:8]
y <- y[1:2000]
```

Project Into Lower Dimensions
=======================================================
1. project with principle components down to 4 dimensions
2. project with that to 2 dimensions with tsne

```{r}
pca <- princomp(X)
X_t <- predict(pca, X)[,1:2]

data <- data.frame(X_t,
                   y = as.factor(y))
data
```

Plot
=======================================================
Plot the components on graph and color by label

```{r}
qplot(Comp.1, Comp.2, color = y, data = data, size = I(.5))
```

Deep Learning
=======================================================
Demo (nn.R)

Building Great Models
=======================================================
* Feature Selection
* Imputing (deal with missing data)
* Model Tuning
* Algorithm Selection
* Clustering and Other Unsupervised Techniques

Suggested Resources
=======================================================
* Coursera Machine Learning Course by Andrew Ng
* Kaggle.com forums and tutorials
* My Machine Learning in R Videos on PacktPub.com

========================================================
![Questions? Feedback?](images/StirTrek2018-SessionSlack.png)

## We are in \#2018--purple
