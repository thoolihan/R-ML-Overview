Machine Learning in R
========================================================
author: Tim Hoolihan (@thoolihan)
date: 2018.07.25
autosize: true
```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
set.seed(1)
```

Tim Hoolihan
========================================================
- Organizer of the Cleveland R User Group
- Sr. Director of Data Science at DialogTech
- Advisory Board Member at DriveIT in Akron
- Creator of a Packt Pub Video Series on Machine Learning in R
- @thoolihan on github & twitter

About the Talk
========================================================

1. Super-brief crash course in R
2. What is machine learning
3. Regression - Predicting Continuous Values
4. Classification - Predicting Discrete(Categorical) Values
5. Unsupervised Learning
5. Deep Learning
6. Your Next Steps

R
========================================================
- An implementation of the S language, that has evolved on it's own
- Built-in support for Statistics
- Lots of packages for a variety of STEM domains
  - Considered a large reason for the growth of the R Community
- Integrations with a lot of systems (Most RDBMSs, Hadoop, Spark, ElasticSearch, etc)
- [r-project.org](r-project.org)

Unique Things About R Syntax
========================================================
Assignment supports `=`, but `<-` is dominant in practice

```{r}
x = 5

# above is the same as below

x <- 5
```

Unique Things About R Syntax
========================================================
Functions declared through assignment, implied return
```{r}
foo <- function(x, y) {
  return(x + y)
}

# above is the same as below

foo <- function(x, y) {
  x + y
}
```

Vectors and Vector Operations
=======================================================
```{r}
heights <- c(1.7, 1.72, 1.68, 1.6, 1.8)
length(heights)
mean(heights)
sd(heights)

```

Vectors and Vector Operations
=======================================================
```{r}
heights <- c(1.7, 1.72, 1.68, 1.6, 1.8)
feet_per_meter <- 3.28

tmp <- heights * feet_per_meter

# above is preferred and more performant than

tmp <- c()
for(height in heights){
  tmp <- c(tmp, height * feet_per_meter)
}
```

Distributions
=======================================================
```{r}
# normal distribution
heights = rnorm(500, mean = 66, sd = 3.5)
heights[1:5]
```

Histogram
=======================================================
```{r}
hist(heights, breaks = 12)
```

Let's make a related variable with some noise
=======================================================
```{r}
# uniform distrubution added to heights
armspans <- heights + runif(500, min = -3, max = 3)
armspans[1:5]
```

Data Frames
=======================================================
```{r}
people <- data.frame(height = heights, armspan = armspans)
head(people)
```

Data Frames
=======================================================
The dim function (dimensions) shows `r dim(people)[1]` observations of `r dim(people)[2]` variables
```{r}
dim(people)
```

Indexing Data Frame
=======================================================
Here's how we examine a column
```{r}
people$height
```

Indexing Data Frame
=======================================================
Here's how we examine a row
```{r}
people[2,]
```

Indexing Data Frame
=======================================================
Here's how we index both a row and column
```{r}
people[2, 'height']
```

Libraries
=======================================================
```{r}
library(ggplot2)
```

Plot Data
=======================================================
```{r}
qplot(x = height, y = armspan, data = people)
```

Syntax Convention
=======================================================
Notice that we didn't say people$height in the x parameter...
```{r, eval=FALSE}
qplot(x = height, y = armspan, data = people)
```

in most languages you would need to do something like this:

```{r, eval=FALSE}
with(people, 
     {qplot(x = height, y = armspan)}
)
```

A lot of functions take a data argument as the environment in which they evaluate the arguments

Formula
=======================================================
- `armspan ~ height` is a formula, can be read as "armspan by height"
```{r}
reg_form <- lm(armspan ~ height, data = people)
```

Summary of our Regression Line
=======================================================
```{r}
summary(reg_form)
```

Regression
=======================================================
- lm is a function to fit a linear model
```{r}
reg_form$coefficients
```
- Remember y = mx + b
  - armspan = `r reg_form$coefficients[['height']]` * height + `r reg_form$coefficients[['(Intercept)']]`

Apply Formula
=======================================================
```{r}
people$rline <- predict(reg_form, newdata = people)
people$rline
```

Plot Regression
=======================================================
```{r}
ggplot(data = people, aes(x = height)) +
  geom_point(aes(y = armspan)) +
  geom_line(aes(y = rline, color = 'regression line'))
```


What is Our Regression Formula/Line?
=======================================================
It describes the relationship between the response variable (armspan) and the dependent variable (height). In other words, it describes how armspan is expected to change when height changes.

How It Was Created
=======================================================
We did not explicitly program the relationship with rules, we used an alogorithm that could infer the relationship based on example data.

Does The Relationship Hold?
=======================================================
Let's assume that was real data from American adults (18-50) measured in the last 10 years...
- Is the relationship valid for adults (18-50) in France?
- Is the relationship valid for children?
- Is the relationship valid for senior citizens?
- Is the relationship valid for ancient Neanderthal people?

This Is Machine Learning
=======================================================
The "rules" of our model weren't program in if-else like statements by a programmer, they were patterns recognized by an algorithm from data.

The model to describe the relationship can be used to to predict the response variable, based on the input variable(s). It's not a perfect predictor, but can be a useful prediction. Among other considerations, it's best that you're trying to predict on a subject that is of the same type as the data used to train the model.

As an example, our model for American adults should not be applied to children, as they were not represented in the set and have meaningful physiological differences.


So Let's Make a Prediction
=======================================================
We meet someone new with a height of 70 inches. What does our model predict their armspan is?
```{r}
new_person = list(height = 70)
predict(reg_form, newdata = new_person)
```

A Word on Data Sets
=======================================================
# When Supervised Learning...
## Train
Data used to train and tune the algorithm

## Validation
Data used to validate the model being built

## Test
Data used with final model to estimate model fit

Caret Package
=======================================================
* Name is short for **C**lassification **A**nd **RE**gression **T**raining

Contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

Regression: Predicting HP
=======================================================
```{r}
head(mtcars)
```

Select Continuous Columns
=======================================================
```{r}
x <- mtcars[, c('hp', 'mpg', 'disp', 'wt', 'qsec')]
head(x)
```

Let's Look At Correlation
=======================================================
```{r}
pairs(x)
```

Split Our Data
=======================================================
```{r}
library(caret)

training_index <- createDataPartition(x$hp, p = 0.7, list = FALSE)
train_data <- x[training_index, ]
test_data <- x[-training_index, ]  # note the '-' in front of the index

dim(train_data)
```

Create Our Model
=======================================================
```{r}
model <- train(hp ~ .,
               data = train_data,
               method = "lm")

summary(model)
```

Predict On Test Data
=======================================================
```{r}
test_data$predicted <- predict(model, test_data)

test_data$miss <- test_data$predicted - test_data$hp

test_data[, c('hp', 'predicted', 'miss')]
```

Measuring Error
=======================================================
```{r}
library(ModelMetrics)
mse(test_data$hp, test_data$predicted)
```

```{r}
mae(test_data$hp, test_data$predicted)
```

Housing Prices
=======================================================
Let's look at a larger size data set:
```{r}
houses_raw <- read.csv("data/housing/train.csv")

houses <- houses_raw[, c('LotArea', 'Neighborhood',
                         'YearBuilt', 'TotRmsAbvGrd',
                         'FullBath', 'HalfBath',
                         'GrLivArea', 'TotalBsmtSF',
                         'SalePrice')]
dim(houses)
```

Categorical Features
=======================================================
```{r}
class(houses$Neighborhood)
```
```{r}
summary(houses$Neighborhood)
```

Cross Validation
=======================================================
![kfold cross val](images/kfold.jpg)

[creative commons - wikimedia]

Cross Validation
=======================================================
```{r}
folds <- 10
trctl <- trainControl(method = "cv", number = folds, savePredictions = TRUE)

model <- train(SalePrice ~ .,
               data = houses,
               method = "lasso",  # another regression model
               trControl = trctl,
               preProcess = c('center', 'scale'))
```

Examine Housing Results
=======================================================
```{r}
mae(houses$SalePrice, model$pred$pred)
```

A Word On Centering
=======================================================
```{r}
center <- function(data) {
  return(data - mean(data))
}

center(0:10)
```

A Word On Scaling
=======================================================
```{r}
scale <- function(data) {
  data <- data - min(data)
  return(data / max(data))
}

scale(0:10)
```

Combined
=======================================================
```{r}
data <- runif(10, min = 1e4, max = 5e6)
data
```

```{r}
center(scale(data))
```

Preprocessing
=======================================================
Caret just handled that with:
```{r, eval = FALSE}
preProcess = c('center', 'scale')
```

Missing Data
=======================================================
Caret can impute missing values:
```{r, eval = FALSE}
preProcess = c('knnImpute')
```

Your mileage may vary with auto-imputing, you will often want to think through how your are imputing

Classification
=======================================================
```{r}
actual <- factor(c('Hero', 'Hydra', 'Shield', 'Cannon Fodder', 'Hydra'))
predicted <- factor(c('Hero', 'Shield', 'Hydra', 'Cannon Fodder', 'Hydra'))
table(actual, predicted)
```

Classification
=======================================================
iris data set
```{r}
head(iris)
```

SVM for Classification
=======================================================
Support Vector Machine
```{r}
trctl <- trainControl(method="cv", repeats=3,
                      savePredictions = TRUE)

model <- train(Species ~ .,
               data = iris,
               method = "svmRadial",
               trControl = trctl,
               preprocess = c('scale'))
```

SVM Results
=======================================================
Confusion Matrix
```{r}
#prefix with library, because ModelMetrics also has a confusion matrix function
caret::confusionMatrix(model$pred$obs, model$pred$pred)$table
```

What Feature mattered?
=======================================================
Caret will assess variable importance:
```{r}
caret::varImp(model)
```

Unsupervised Learning
=======================================================
The algorithm looks for patterns without answers/labels. This is useful for exploring your data.

Clustering with labelled classification data before doing supervised learning, can be a great way to decide if the categories you selected may have overlap, or other issues.

Projection
=======================================================

```{r, echo = FALSE}
qplot(x = hp, y = qsec, color = as.factor(cyl), data = mtcars,
      main = "Horsepower vs 1/4 second time")
```

***

```{r, echo = FALSE}
hps <- mtcars[, c('hp', 'qsec')]
pca <- princomp(hps)
mtcars$pca <- predict(pca, hps)[,1]
mtcars$zero <- mtcars$hp * 0
qplot(x = pca, y = zero, color = as.factor(cyl), data = mtcars,
      main = "Projected to 1-dimension")
```

Unsupervised Learning
=======================================================
Let's load up Satellite image data that could be used in a classification problem

```{r}
library(mlbench)
library(tsne)

data("Satellite")
dim(Satellite)
```

Clustering Satellite Data
=======================================================
What fields to do we have?

```{r}
names(Satellite)
```

Clustering Satellite Data
=======================================================
What does our input data look like?

```{r}
class(Satellite$x.10)
```


```{r}
summary(Satellite$x.10)
```

Clustering Satellite Data
=======================================================
What does our label data look like?

```{r}
class(Satellite$classes)
```


```{r}
summary(Satellite$classes)
```

Split into X, y
=======================================================
In preparation for projecting...

We're going to use principle components and tsne to create a 2d projection of 36 dimensional data.

```{r}
X <- Satellite[, -37] #drop last column
y <- Satellite$classes
```

```{r, echo = FALSE}
X <- X[1:2000, 1:8]
y <- y[1:2000]
```

Project Into Lower Dimensions
=======================================================
1. project with principle components down to 4 dimensions
2. project with that to 2 dimensions with tsne

```{r}
pca <- princomp(X)
X_t <- predict(pca, X)[,1:2]

data <- data.frame(X_t,
                   y = as.factor(y))
data
```

Plot
=======================================================
Plot the components on graph and color by label

```{r}
qplot(Comp.1, Comp.2, color = y, data = data, size = I(.5))
```

Pipes
=======================================================

```{r, eval = FALSE}
# Normal Syntax
a <- 1:5

sample(a, size = 2)
```

can be written as...

```{r, eval=FALSE}
# Pipe Sytax

a %>% sample(size = 2)
```

What's the Benefit?
=======================================================
Using dplyr for data manipulation:
```{r, eval = FALSE}
flights %>%
  group_by(year, month, day) %>%
  select(arr_delay, dep_delay) %>%
  summarise(
    arr = mean(arr_delay, na.rm = TRUE),
    dep = mean(dep_delay, na.rm = TRUE)
  ) %>%
  filter(arr > 30 | dep > 30)
```

Deep Learning
=======================================================
Demo (nn.R)

Building Great Models
=======================================================
* Feature Selection
* Imputing (deal with missing data)
* Model Tuning
* Algorithm Selection
* Statistical Learning

Suggested Resources
=======================================================
* Coursera Machine Learning Course by Andrew Ng
* Kaggle.com forums and tutorials
* Machine Learning in R Videos on PacktPub.com
  * [packtpub.com/all?search=Hoolihan](https://www.packtpub.com/all?search=Hoolihan)

Questions? Feedback?
========================================================

### Slides are at https://github.com/thoolihan/R-ML-Overview
